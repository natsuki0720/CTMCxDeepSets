{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from likelihood import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from data_generator import *\n",
    "from formate_matrix_toMLData import * \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import Attention_model \n",
    "from model_scripts.proto_type.prototype_03.DeepSets_varSet_diagnal import *\n",
    "# from model_scripts.diagonal_inverse_1to100.DeepSets_varSet_diagnal import DeepSets_varSets_forDiagnel, varSets_Datasets, collate_fn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データセットように整形\n",
    "states = []\n",
    "del_t = []\n",
    "outputs = []\n",
    "formater = formate_dataMatrix()\n",
    "\n",
    "#データ生成\n",
    "# for i in range(14355,20000):\n",
    "#     DTRMG = DiagonalTransitionRateMatrixGenerator(4)\n",
    "#     Q = DTRMG.generateMatrix()\n",
    "#     dg = DataGenerator(Q, 50000)\n",
    "#     all_matrix = dg.generate_matrix()\n",
    "#     dg.generate_dataFile(all_matrix,f\"data_{str(i)}_\", \"data/diagonal_setSize50000\")\n",
    "#     tm = matrix_trimer(all_matrix)\n",
    "#     trm = tm.trim_transitionRateMatrix()\n",
    "#     data = tm.trim_data()\n",
    "#     output_vec = np.array(formater.GetOutputVector_byUpperTriangle(trm))\n",
    "#     # states に追加（各要素は (2, num_samples_i) の形状）\n",
    "#     state = np.stack([data[:,0], data[:,1]], axis=0)# shape: (2, num_samples_i)\n",
    "#     states.append(state)\n",
    "#     # del_t に追加（各要素は (num_samples_i,) の形状）\n",
    "#     del_t.append(data[:, 2])\n",
    "#     # outputs に追加（各要素は (6,) の形状）\n",
    "#     outputs.append(output_vec)\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "del_t = []\n",
    "outputs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_file(file_path):\n",
    "    #ファイルの読み込み\n",
    "    with open(file_path,encoding=\"utf-8\") as f:\n",
    "        all_matrix = np.loadtxt(f, delimiter= \",\")\n",
    "    tm = matrix_trimer(all_matrix)\n",
    "    trm = tm.trim_transitionRateMatrix()\n",
    "    data = tm.trim_data()\n",
    "    output_vec = np.array(formater.GetOutputVector_byDiagonal(trm))\n",
    "    # data: shape (num_samples_i, 3)\n",
    "    # data[:, 0]: pre 状態\n",
    "    # data[:, 1]: post 状態\n",
    "    # data[:, 2]: delta_t\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # states に追加（各要素は (2, num_samples_i) の形状）\n",
    "    state = np.stack([data[:,0], data[:,1]], axis=0)# shape: (2, num_samples_i)\n",
    "    states.append(state)\n",
    "    \n",
    "    # del_t に追加（各要素は (num_samples_i,) の形状）\n",
    "    del_t.append(data[:, 2])\n",
    "    \n",
    "    # outputs に追加（各要素は (3,) の形状）\n",
    "    outputs.append(output_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formater.process_all_files_in_directory(\"/Users/yamashitanatsuki/Desktop/data/diagonal_50000_val_ratio\", process_file)\n",
    "formater.process_all_files_in_directory(\"/Users/yamashitanatsuki/Desktop/data/diagonal_50000_val_ratio04_uniform_life\",process_file,end=50000)\n",
    "# データの分割\n",
    "# 1. トレーニング＋検証 と テスト に分割\n",
    "train_states, val_states, train_del_t, val_del_t, train_outputs, val_outputs = train_test_split(\n",
    "    states, del_t, outputs, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# 2. トレーニング と 検証 に分割\n",
    "# train_states, val_states, train_del_t, val_del_t, train_outputs, val_outputs = train_test_split(\n",
    "#     train_val_states, train_val_del_t, train_val_outputs, test_size=0.25, random_state=42\n",
    "# )\n",
    "\n",
    "# データサイズの確認\n",
    "print(f\"Total data size: {len(states)}\")\n",
    "print(f\"Training data size: {len(train_states)}\")\n",
    "print(f\"Validation data size: {len(val_states)}\")\n",
    "# print(f\"Test data size: {len(test_states)}\")\n",
    "\n",
    "\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "train_dataset = varSets_Datasets(train_states, train_del_t, train_outputs)\n",
    "val_dataset = varSets_Datasets(val_states, val_del_t, val_outputs)\n",
    "# test_dataset = varSets_Datasets(test_states, test_del_t, test_outputs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a, b, c, d in train_loader:\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = Attention_model.DeepSets_AttentionAggregate(num_categories=4, embedding_dim=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "class CustomLoss(nn.Module):\n",
    "    def forward(self, outputs, targets):\n",
    "         # 安全のため、小さい値を追加してゼロ割りを防止\n",
    "        epsilon = 1e-12\n",
    "        y_pred_inverse = 1.0 / (outputs + epsilon)\n",
    "        y_true_inverse = 1.0 / (targets + epsilon)\n",
    "        \n",
    "        loss = torch.abs(y_pred_inverse-y_true_inverse).mean()\n",
    "        \n",
    "        # 平均損失を計算\n",
    "        return loss\n",
    "criterion = CustomLoss()\n",
    "\n",
    "class KLloss(nn.Module):\n",
    "    def forward(self, outputs,targets):\n",
    "        kl_div = torch.log(targets / outputs) + (outputs / targets)\n",
    "        return kl_div.sum()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 30.481714\n",
      "Validation Loss: 0.211278\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 2/10, Training Loss: 27.299786\n",
      "Validation Loss: 0.193623\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 3/10, Training Loss: 25.221632\n",
      "Validation Loss: 0.169558\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 4/10, Training Loss: 23.382277\n",
      "Validation Loss: 0.139821\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 5/10, Training Loss: 20.177115\n",
      "Validation Loss: 0.128122\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 6/10, Training Loss: 18.888176\n",
      "Validation Loss: 0.127082\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 7/10, Training Loss: 18.036740\n",
      "Validation Loss: 0.123679\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 8/10, Training Loss: 16.587644\n",
      "Validation Loss: 0.083750\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 9/10, Training Loss: 14.567283\n",
      "Validation Loss: 0.073733\n",
      "Validation loss improved. Model saved.\n",
      "Epoch 10/10, Training Loss: 13.490118\n",
      "Validation Loss: 0.092267\n",
      "No improvement in validation loss for 1 epochs.\n"
     ]
    }
   ],
   "source": [
    "# 早期終了の設定\n",
    "num_epochs = 10  # 最大エポック数\n",
    "patience = 10     # 検証損失が改善しない許容エポック\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "# loss_func = KLloss()\n",
    "best_model_path = 'Attention_model.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for states_batch, del_t_batch, targets_batch, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outs = model(states_batch,del_t_batch, lengths)\n",
    "        loss = criterion(outs, targets_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * states_batch.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "    # 検証ループ\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_states_batch, val_del_t_batch, val_targets_batch, val_lengths in val_loader:\n",
    "            val_outs = model(val_states_batch, val_del_t_batch, val_lengths)\n",
    "            val_loss = criterion(val_outs, val_targets_batch)\n",
    "            val_running_loss += val_loss.item() \n",
    "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.6f}\")\n",
    "\n",
    "    # 検証損失の改善を確認\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        patience_counter = 0\n",
    "        # 最良モデルの保存\n",
    "        # --- 保存 ---\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        print(\"Validation loss improved. Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation loss for {patience_counter} epochs.\")\n",
    "\n",
    "    # 早期終了の判定\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# test_running_loss = 0\n",
    "# with torch.no_grad():\n",
    "#     for test_states_batch, test_del_t_batch, test_target_batch, test_lengths in test_loader:\n",
    "#         test_outs = model(test_states_batch, test_del_t_batch,test_lengths)\n",
    "#         test_loss = criterion(test_outs, test_target_batch)\n",
    "#         test_running_loss += test_loss.item() * test_target_batch.size(1)\n",
    "        \n",
    "# test_epoch_loss = test_running_loss/len(test_dataset)\n",
    "# print(f\"test Loss: {val_epoch_loss:.6f}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model_scripts.diagonal_var_initial_ratio_02.DeepSets_varSet_diagnal  as m\n",
    "# # 早期終了の設定\n",
    "# num_epochs = 1000  # 最大エポック数\n",
    "# patience = 10     # 検証損失が改善しない許容エポック数\n",
    "# best_val_loss = float(\"inf\")\n",
    "# patience_counter = 0\n",
    "# model = m.DeepSets_varSets_forDiagnel(3,2)\n",
    "# best_model_path = 'model_scripts/diagonal_var_initial_ratio_02/diagonal_var_ratio_.pth'\n",
    "# loss_func = CustomLoss()\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for states_batch, del_t_batch, targets_batch, lengths in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outs = model(states_batch,del_t_batch, lengths)\n",
    "#         loss = criterion(outs, targets_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item() * states_batch.size(0)\n",
    "#     epoch_loss = running_loss / len(train_dataset)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "#     # 検証ループ\n",
    "#     model.eval()\n",
    "#     val_running_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for val_states_batch, val_del_t_batch, val_targets_batch, val_lengths in val_loader:\n",
    "#             val_outs = model(val_states_batch, val_del_t_batch, val_lengths)\n",
    "#             val_loss = criterion(val_outs, val_targets_batch)\n",
    "#             val_running_loss += val_loss.item() \n",
    "#     val_epoch_loss = val_running_loss / len(val_dataset)\n",
    "#     print(f\"Validation Loss: {val_epoch_loss:.6f}\")\n",
    "\n",
    "#     # 検証損失の改善を確認\n",
    "#     if val_epoch_loss < best_val_loss:\n",
    "#         best_val_loss = val_epoch_loss\n",
    "#         patience_counter = 0\n",
    "#         # 最良モデルの保存\n",
    "#         torch.save(model, best_model_path)\n",
    "#         print(\"Validation loss improved. Model saved.\")\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         print(f\"No improvement in validation loss for {patience_counter} epochs.\")\n",
    "\n",
    "#     # 早期終了の判定\n",
    "#     if patience_counter >= patience:\n",
    "#         print(\"Early stopping triggered.\")\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kisozemi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
